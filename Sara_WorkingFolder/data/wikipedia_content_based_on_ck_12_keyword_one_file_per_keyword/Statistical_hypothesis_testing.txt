a statistical hypothesis is a hypothesis that is testable on the basis of observing a process that is modeled via a set of random variables a statistical hypothesis test is a method of statistical inference commonly two statistical data sets are compared or a data set obtained by sampling is compared against a synthetic data set from an idealized model a hypothesis is proposed for the statistical relationship between the two data sets and this is compared as an alternative to an idealized null hypothesis that proposes no relationship between two data sets the comparison is deemed statistically significant if the relationship between the data sets would be an unlikely realization of the null hypothesis according to a threshold probabilitythe significance level hypothesis tests are used in determining what outcomes of a study would lead to a rejection of the null hypothesis for a pre-specified level of significance the process of distinguishing between the null hypothesis and the alternative hypothesis is aided by identifying two conceptual types of errors (type 1 & type 2) and by specifying parametric limits on e.g how much type 1 error will be permitted
an alternative framework for statistical hypothesis testing is to specify a set of statistical models one for each candidate hypothesis and then use model selection techniques to choose the most appropriate model the most common selection techniques are based on either akaike information criterion or bayes factor
statistical hypothesis testing is sometimes called confirmatory data analysis it can be contrasted with exploratory data analysis which may not have pre-specified hypotheses
== variations and sub-classes ==
statistical hypothesis testing is a key technique of both frequentist inference and bayesian inference although the two types of inference have notable differences statistical hypothesis tests define a procedure that controls (fixes) the probability of incorrectly deciding that a default position (null hypothesis) is incorrect the procedure is based on how likely it would be for a set of observations to occur if the null hypothesis were true note that this probability of making an incorrect decision is not the probability that the null hypothesis is true nor whether any specific alternative hypothesis is true this contrasts with other possible techniques of decision theory in which the null and alternative hypothesis are treated on a more equal basis
one naive bayesian approach to hypothesis testing is to base decisions on the posterior probability but this fails when comparing point and continuous hypotheses other approaches to decision making such as bayesian decision theory attempt to balance the consequences of incorrect decisions across all possibilities rather than concentrating on a single null hypothesis a number of other approaches to reaching a decision based on data are available via decision theory and optimal decisions some of which have desirable properties hypothesis testing though is a dominant approach to data analysis in many fields of science extensions to the theory of hypothesis testing include the study of the power of tests i.e the probability of correctly rejecting the null hypothesis given that it is false such considerations can be used for the purpose of sample size determination prior to the collection of data
== the testing process ==
in the statistics literature statistical hypothesis testing plays a fundamental role the usual line of reasoning is as follows:
there is an initial research hypothesis of which the truth is unknown
the first step is to state the relevant null and alternative hypotheses this is important as mis-stating the hypotheses will muddy the rest of the process
the second step is to consider the statistical assumptions being made about the sample in doing the test; for example assumptions about the statistical independence or about the form of the distributions of the observations this is equally important as invalid assumptions will mean that the results of the test are invalid
decide which test is appropriate and state the relevant test statistic t
derive the distribution of the test statistic under the null hypothesis from the assumptions in standard cases this will be a well-known result for example the test statistic might follow a student's t distribution or a normal distribution
select a significance level () a probability threshold below which the null hypothesis will be rejected common values are 5% and 1%
the distribution of the test statistic under the null hypothesis partitions the possible values of t into those for which the null hypothesis is rejectedthe so-called critical regionand those for which it is not the probability of the critical region is 
compute from the observations the observed value tobs of the test statistic t
decide to either reject the null hypothesis in favor of the alternative or not reject it the decision rule is to reject the null hypothesis h0 if the observed value tobs is in the critical region and to accept or "fail to reject" the hypothesis otherwise
an alternative process is commonly used:
compute from the observations the observed value tobs of the test statistic t
calculate the p-value this is the probability under the null hypothesis of sampling a test statistic at least as extreme as that which was observed
reject the null hypothesis in favor of the alternative hypothesis if and only if the p-value is less than the significance level (the selected probability) threshold
the two processes are equivalent the former process was advantageous in the past when only tables of test statistics at common probability thresholds were available it allowed a decision to be made without the calculation of a probability it was adequate for classwork and for operational use but it was deficient for reporting results
the latter process relied on extensive tables or on computational support not always available the explicit calculation of a probability is useful for reporting the calculations are now trivially performed with appropriate software
the difference in the two processes applied to the radioactive suitcase example (below):
"the geiger-counter reading is 10 the limit is 9 check the suitcase."
"the geiger-counter reading is high; 97% of safe suitcases have lower readings the limit is 95% check the suitcase."
the former report is adequate the latter gives a more detailed explanation of the data and the reason why the suitcase is being checked
it is important to note the difference between accepting the null hypothesis and simply failing to reject it the "fail to reject" terminology highlights the fact that the null hypothesis is assumed to be true from the start of the test; if there is a lack of evidence against it it simply continues to be assumed true the phrase "accept the null hypothesis" may suggest it has been proved simply because it has not been disproved a logical fallacy known as the argument from ignorance unless a test with particularly high power is used the idea of "accepting" the null hypothesis may be dangerous nonetheless the terminology is prevalent throughout statistics where its meaning is well understood
the processes described here are perfectly adequate for computation they seriously neglect the design of experiments considerations
it is particularly critical that appropriate sample sizes be estimated before conducting the experiment
the phrase "test of significance" was coined by statistician ronald fisher
=== interpretation ===
if the p-value is less than the required significance level (equivalently if the observed test statistic is in the critical region) then we say the null hypothesis is rejected at the given level of significance rejection of the null hypothesis is a conclusion this is like a "guilty" verdict in a criminal trial: the evidence is sufficient to reject innocence thus proving guilt we might accept the alternative hypothesis (and the research hypothesis)
if the p-value is not less than the required significance level (equivalently if the observed test statistic is outside the critical region) then the test has no result the evidence is insufficient to support a conclusion (this is like a jury that fails to reach a verdict.) the researcher typically gives extra consideration to those cases where the p-value is close to the significance level
in the lady tasting tea example (below) fisher required the lady to properly categorize all of the cups of tea to justify the conclusion that the result was unlikely to result from chance he defined the critical region as that case alone the region was defined by a probability (that the null hypothesis was correct) of less than 5%
whether rejection of the null hypothesis truly justifies acceptance of the research hypothesis depends on the structure of the hypotheses rejecting the hypothesis that a large paw print originated from a bear does not immediately prove the existence of bigfoot hypothesis testing emphasizes the rejection which is based on a probability rather than the acceptance which requires extra steps of logic
"the probability of rejecting the null hypothesis is a function of five factors: whether the test is one- or two tailed the level of significance the standard deviation the amount of deviation from the null hypothesis and the number of observations." these factors are a source of criticism; factors under the control of the experimenter/analyst give the results an appearance of subjectivity
=== use and importance ===
statistics are helpful in analyzing most collections of data this is equally true of hypothesis testing which can justify conclusions even when no scientific theory exists in the lady tasting tea example it was "obvious" that no difference existed between (milk poured into tea) and (tea poured into milk) the data contradicted the "obvious"
real world applications of hypothesis testing include:
testing whether more men than women suffer from nightmares
establishing authorship of documents
evaluating the effect of the full moon on behavior
determining the range at which a bat can detect an insect by echo
deciding whether hospital carpeting results in more infections
selecting the best means to stop smoking
checking whether bumper stickers reflect car owner behavior
testing the claims of handwriting analysts
statistical hypothesis testing plays an important role in the whole of statistics and in statistical inference for example lehmann (1992) in a review of the fundamental paper by neyman and pearson (1933) says: "nevertheless despite their shortcomings the new paradigm formulated in the 1933 paper and the many developments carried out within its framework continue to play a central role in both the theory and practice of statistics and can be expected to do so in the foreseeable future"
significance testing has been the favored statistical tool in some experimental social sciences (over 90% of articles in the journal of applied psychology during the early 1990s) other fields have favored the estimation of parameters (e.g. effect size) significance testing is used as a substitute for the traditional comparison of predicted value and experimental result at the core of the scientific method when theory is only capable of predicting the sign of a relationship a directional (one-sided) hypothesis test can be configured so that only a statistically significant result supports theory this form of theory appraisal is the most heavily criticized application of hypothesis testing
=== cautions ===
"if the government required statistical procedures to carry warning labels like those on drugs most inference methods would have long labels indeed." this caution applies to hypothesis tests and alternatives to them
the successful hypothesis test is associated with a probability and a type-i error rate the conclusion might be wrong
the conclusion of the test is only as solid as the sample upon which it is based the design of the experiment is critical a number of unexpected effects have been observed including:
the clever hans effect a horse appeared to be capable of doing simple arithmetic
the hawthorne effect industrial workers were more productive in better illumination and most productive in worse
the placebo effect pills with no medically active ingredients were remarkably effective
a statistical analysis of misleading data produces misleading conclusions the issue of data quality can be more subtle in forecasting for example there is no agreement on a measure of forecast accuracy in the absence of a consensus measurement no decision based on measurements will be without controversy
the book how to lie with statistics is the most popular book on statistics ever published it does not much consider hypothesis testing but its cautions are applicable including: many claims are made on the basis of samples too small to convince if a report does not mention sample size be doubtful
hypothesis testing acts as a filter of statistical conclusions; only those results meeting a probability threshold are publishable economics also acts as a publication filter; only those results favorable to the author and funding source may be submitted for publication the impact of filtering on publication is termed publication bias a related problem is that of multiple testing (sometimes linked to data mining) in which a variety of tests for a variety of possible effects are applied to a single data set and only those yielding a significant result are reported these are often dealt with by using multiplicity correction procedures that control the family wise error rate (fwer) or the false discovery rate (fdr)
those making critical decisions based on the results of a hypothesis test are prudent to look at the details rather than the conclusion alone in the physical sciences most results are fully accepted only when independently confirmed the general advice concerning statistics is "figures never lie but liars figure" (anonymous)
== examples ==
=== lady tasting tea ===
in a famous example of hypothesis testing known as the lady tasting tea a female colleague of fisher claimed to be able to tell whether the tea or the milk was added first to a cup fisher proposed to give her eight cups four of each variety in random order one could then ask what the probability was for her getting the number she got correct but just by chance the null hypothesis was that the lady had no such ability the test statistic was a simple count of the number of successes in selecting the 4 cups the critical region was the single case of 4 successes of 4 possible based on a conventional probability criterion (< 5%; 1 of 70 1.4%) fisher asserted that no alternative hypothesis was (ever) required the lady correctly identified every cup which would be considered a statistically significant result
=== courtroom trial ===
a statistical test procedure is comparable to a criminal trial; a defendant is considered not guilty as long as his or her guilt is not proven the prosecutor tries to prove the guilt of the defendant only when there is enough charging evidence the defendant is convicted
in the start of the procedure there are two hypotheses : "the defendant is not guilty" and : "the defendant is guilty" the first one is called null hypothesis and is for the time being accepted the second one is called alternative (hypothesis) it is the hypothesis one hopes to support
the hypothesis of innocence is only rejected when an error is very unlikely because one doesn't want to convict an innocent defendant such an error is called error of the first kind (i.e. the conviction of an innocent person) and the occurrence of this error is controlled to be rare as a consequence of this asymmetric behaviour the error of the second kind (acquitting a person who committed the crime) is often rather large
a criminal trial can be regarded as either or both of two decision processes: guilty vs not guilty or evidence vs a threshold ("beyond a reasonable doubt") in one view the defendant is judged; in the other view the performance of the prosecution (which bears the burden of proof) is judged a hypothesis test can be regarded as either a judgment of a hypothesis or as a judgment of evidence
=== philosopher's beans ===
the following example was produced by a philosopher describing scientific methods generations before hypothesis testing was formalized and popularized
few beans of this handful are white
most beans in this bag are white
therefore: probably these beans were taken from another bag
this is an hypothetical inference
the beans in the bag are the population the handful are the sample the null hypothesis is that the sample originated from the population the criterion for rejecting the null-hypothesis is the "obvious" difference in appearance (an informal difference in the mean) the interesting result is that consideration of a real population and a real sample produced an imaginary bag the philosopher was considering logic rather than probability to be a real statistical hypothesis test this example requires the formalities of a probability calculation and a comparison of that probability to a standard
a simple generalization of the example considers a mixed bag of beans and a handful that contain either very few or very many white beans the generalization considers both extremes it requires more calculations and more comparisons to arrive at a formal answer but the core philosophy is unchanged; if the composition of the handful is greatly different from that of the bag then the sample probably originated from another bag the original example is termed a one-sided or a one-tailed test while the generalization is termed a two-sided or two-tailed test
the statement also relies on the inference that the sampling was random if someone had been picking through the bag to find white beans then it would explain why the handful had so many white beans and also explain why the number of white beans in the bag was depleted (although the bag is probably intended to be assumed much larger than one's hand)
=== clairvoyant card game ===
a person (the subject) is tested for clairvoyance he is shown the reverse of a randomly chosen playing card 25 times and asked which of the four suits it belongs to the number of hits or correct answers is called x
as we try to find evidence of his clairvoyance for the time being the null hypothesis is that the person is not clairvoyant the alternative is of course: the person is (more or less) clairvoyant
if the null hypothesis is valid the only thing the test person can do is guess for every card the probability (relative frequency) of any single suit appearing is 1/4 if the alternative is valid the test subject will predict the suit correctly with probability greater than 1/4 we will call the probability of guessing correctly p the hypotheses then are:
null hypothesis (just guessing)
and
alternative hypothesis (true clairvoyant)
when the test subject correctly predicts all 25 cards we will consider him clairvoyant and reject the null hypothesis thus also with 24 or 23 hits with only 5 or 6 hits on the other hand there is no cause to consider him so but what about 12 hits or 17 hits what is the critical number c of hits at which point we consider the subject to be clairvoyant how do we determine the critical value c it is obvious that with the choice c=25 (i.e we only accept clairvoyance when all cards are predicted correctly) we're more critical than with c=10 in the first case almost no test subjects will be recognized to be clairvoyant in the second case a certain number will pass the test in practice one decides how critical one will be that is one decides how often one accepts an error of the first kind a false positive or type i error with c = 25 the probability of such an error is:
and hence very small the probability of a false positive is the probability of randomly guessing correctly all 25 times
being less critical with c=10 gives:
thus c = 10 yields a much greater probability of false positive
before the test is actually performed the maximum acceptable probability of a type i error () is determined typically values in the range of 1% to 5% are selected (if the maximum acceptable error rate is zero an infinite number of correct guesses is required.) depending on this type 1 error rate the critical value c is calculated for example if we select an error rate of 1% c is calculated thus:
from all the numbers c with this property we choose the smallest in order to minimize the probability of a type ii error a false negative for the above example we select: 
=== radioactive suitcase ===
as an example consider determining whether a suitcase contains some radioactive material placed under a geiger counter it produces 10 counts per minute the null hypothesis is that no radioactive material is in the suitcase and that all measured counts are due to ambient radioactivity typical of the surrounding air and harmless objects we can then calculate how likely it is that we would observe 10 counts per minute if the null hypothesis were true if the null hypothesis predicts (say) on average 9 counts per minute then according to the poisson distribution typical for radioactive decay there is about 41% chance of recording 10 or more counts thus we can say that the suitcase is compatible with the null hypothesis (this does not guarantee that there is no radioactive material just that we don't have enough evidence to suggest there is) on the other hand if the null hypothesis predicts 3 counts per minute (for which the poisson distribution predicts only 0.1% chance of recording 10 or more counts) then the suitcase is not compatible with the null hypothesis and there are likely other factors responsible to produce the measurements
the test does not directly assert the presence of radioactive material a successful test asserts that the claim of no radioactive material present is unlikely given the reading (and therefore ) the double negative (disproving the null hypothesis) of the method is confusing but using a counter-example to disprove is standard mathematical practice the attraction of the method is its practicality we know (from experience) the expected range of counts with only ambient radioactivity present so we can say that a measurement is unusually large statistics just formalizes the intuitive by using numbers instead of adjectives we probably do not know the characteristics of the radioactive suitcases; we just assume that they produce larger readings
to slightly formalize intuition: radioactivity is suspected if the geiger-count with the suitcase is among or exceeds the greatest (5% or 1%) of the geiger-counts made with ambient radiation alone this makes no assumptions about the distribution of counts many ambient radiation observations are required to obtain good probability estimates for rare events
the test described here is more fully the null-hypothesis statistical significance test the null hypothesis represents what we would believe by default before seeing any evidence statistical significance is a possible finding of the test declared when the observed sample is unlikely to have occurred by chance if the null hypothesis were true the name of the test describes its formulation and its possible outcome one characteristic of the test is its crisp decision: to reject or not reject the null hypothesis a calculated value is compared to a threshold which is determined from the tolerable risk of error
== definition of terms ==
the following definitions are mainly based on the exposition in the book by lehmann and romano:
statistical hypothesis
a statement about the parameters describing a population (not a sample)
statistic
a value calculated from a sample often to summarize the sample for comparison purposes
simple hypothesis
any hypothesis which specifies the population distribution completely
composite hypothesis
any hypothesis which does not specify the population distribution completely
null hypothesis (h0)
a simple hypothesis associated with a contradiction to a theory one would like to prove
alternative hypothesis (h1)
a hypothesis (often composite) associated with a theory one would like to prove
statistical test
a procedure whose inputs are samples and whose result is a hypothesis
region of acceptance
the set of values of the test statistic for which we fail to reject the null hypothesis
region of rejection / critical region
the set of values of the test statistic for which the null hypothesis is rejected
critical value
the threshold value delimiting the regions of acceptance and rejection for the test statistic
power of a test (1 )
the test's probability of correctly rejecting the null hypothesis the complement of the false negative rate  power is termed sensitivity in biostatistics ("this is a sensitive test because the result is negative we can confidently say that the patient does not have the condition.") see sensitivity and specificity and type i and type ii errors for exhaustive definitions
size
for simple hypotheses this is the test's probability of incorrectly rejecting the null hypothesis the false positive rate for composite hypotheses this is the supremum of the probability of rejecting the null hypothesis over all cases covered by the null hypothesis the complement of the false positive rate is termed specificity in biostatistics ("this is a specific test because the result is positive we can confidently say that the patient has the condition.") see sensitivity and specificity and type i and type ii errors for exhaustive definitions
significance level of a test ()
it is the upper bound imposed on the size of a test its value is chosen by the statistician prior to looking at the data or choosing any particular test to be used it is the maximum exposure to erroneously rejecting h0 he/she is ready to accept testing h0 at significance level means testing h0 with a test whose size does not exceed  in most cases one uses tests whose size is equal to the significance level
p-value
the probability assuming the null hypothesis is true of observing a result at least as extreme as the test statistic
statistical significance test
a predecessor to the statistical hypothesis test (see the origins section) an experimental result was said to be statistically significant if a sample was sufficiently inconsistent with the (null) hypothesis this was variously considered common sense a pragmatic heuristic for identifying meaningful experimental results a convention establishing a threshold of statistical evidence or a method for drawing conclusions from data the statistical hypothesis test added mathematical rigor and philosophical consistency to the concept by making the alternative hypothesis explicit the term is loosely used to describe the modern version which is now part of statistical hypothesis testing
conservative test
a test is conservative if when constructed for a given nominal significance level the true probability of incorrectly rejecting the null hypothesis is never greater than the nominal level
exact test
a test in which the significance level or critical value can be computed exactly i.e. without any approximation in some contexts this term is restricted to tests applied to categorical data and to permutation tests in which computations are carried out by complete enumeration of all possible outcomes and their probabilities
a statistical hypothesis test compares a test statistic (z or t for examples) to a threshold the test statistic (the formula found in the table below) is based on optimality for a fixed level of type i error rate use of these statistics minimizes type ii error rates (equivalent to maximizing power) the following terms describe tests in terms of such optimality:
most powerful test
for a given size or significance level the test with the greatest power (probability of rejection) for a given value of the parameter(s) being tested contained in the alternative hypothesis
uniformly most powerful test (ump)
a test with the greatest power for all values of the parameter(s) being tested contained in the alternative hypothesis
== common test statistics ==
one-sample tests are appropriate when a sample is being compared to the population from a hypothesis the population characteristics are known from theory or are calculated from the population
two-sample tests are appropriate for comparing two samples typically experimental and control samples from a scientifically controlled experiment
paired tests are appropriate for comparing two samples where it is impossible to control important variables rather than comparing two sets members are paired between samples so the difference between the members becomes the sample typically the mean of the differences is then compared to zero the common example scenario for when a paired difference test is appropriate is when a single set of test subjects has something applied to them and the test is intended to check for an effect
z-tests are appropriate for comparing means under stringent conditions regarding normality and a known standard deviation
a t-test is appropriate for comparing means under relaxed conditions (less is assumed)
tests of proportions are analogous to tests of means (the 50% proportion)
chi-squared tests use the same calculations and the same probability distribution for different applications:
chi-squared tests for variance are used to determine whether a normal population has a specified variance the null hypothesis is that it does
chi-squared tests of independence are used for deciding whether two variables are associated or are independent the variables are categorical rather than numeric it can be used to decide whether left-handedness is correlated with libertarian politics (or not) the null hypothesis is that the variables are independent the numbers used in the calculation are the observed and expected frequencies of occurrence (from contingency tables)
chi-squared goodness of fit tests are used to determine the adequacy of curves fit to data the null hypothesis is that the curve fit is adequate it is common to determine curve shapes to minimize the mean square error so it is appropriate that the goodness-of-fit calculation sums the squared errors
f-tests (analysis of variance anova) are commonly used when deciding whether groupings of data by category are meaningful if the variance of test scores of the left-handed in a class is much smaller than the variance of the whole class then it may be useful to study lefties as a group the null hypothesis is that two variances are the same so the proposed grouping is not meaningful
in the table below the symbols used are defined at the bottom of the table many other tests can be found in other articles proofs exist that the test statistics are appropriate
== origins and early controversy ==
significance testing is largely the product of karl pearson (p-value pearson's chi-squared test) william sealy gosset (student's t-distribution) and ronald fisher ("null hypothesis" analysis of variance "significance test") while hypothesis testing was developed by jerzy neyman and egon pearson (son of karl) ronald fisher began his life in statistics as a bayesian (zabell 1992) but fisher soon grew disenchanted with the subjectivity involved (namely use of the principle of indifference when determining prior probabilities) and sought to provide a more "objective" approach to inductive inference
fisher was an agricultural statistician who emphasized rigorous experimental design and methods to extract a result from few samples assuming gaussian distributions neyman (who teamed with the younger pearson) emphasized mathematical rigor and methods to obtain more results from many samples and a wider range of distributions modern hypothesis testing is an inconsistent hybrid of the fisher vs neyman/pearson formulation methods and terminology developed in the early 20th century while hypothesis testing was popularized early in the 20th century evidence of its use can be found much earlier in the 1770s laplace considered the statistics of almost half a million births the statistics showed an excess of boys compared to girls he concluded by calculation of a p-value that the excess was a real but unexplained effect
fisher popularized the "significance test" he required a null-hypothesis (corresponding to a population frequency distribution) and a sample his (now familiar) calculations determined whether to reject the null-hypothesis or not significance testing did not utilize an alternative hypothesis so there was no concept of a type ii error
the p-value was devised as an informal but objective index meant to help a researcher determine (based on other knowledge) whether to modify future experiments or strengthen one's faith in the null hypothesis hypothesis testing (and type i/ii errors) was devised by neyman and pearson as a more objective alternative to fisher's p-value also meant to determine researcher behaviour but without requiring any inductive inference by the researcher
neyman & pearson considered a different problem (which they called "hypothesis testing") they initially considered two simple hypotheses (both with frequency distributions) they calculated two probabilities and typically selected the hypothesis associated with the higher probability (the hypothesis more likely to have generated the sample) their method always selected a hypothesis it also allowed the calculation of both types of error probabilities
fisher and neyman/pearson clashed bitterly neyman/pearson considered their formulation to be an improved generalization of significance testing.(the defining paper was abstract mathematicians have generalized and refined the theory for decades.) fisher thought that it was not applicable to scientific research because often during the course of the experiment it is discovered that the initial assumptions about the null hypothesis are questionable due to unexpected sources of error he believed that the use of rigid reject/accept decisions based on models formulated before data is collected was incompatible with this common scenario faced by scientists and attempts to apply this method to scientific research would lead to mass confusion
the dispute between fisher and neymanpearson was waged on philosophical grounds characterized by a philosopher as a dispute over the proper role of models in statistical inference
events intervened: neyman accepted a position in the western hemisphere breaking his partnership with pearson and separating disputants (who had occupied the same building) by much of the planetary diameter world war ii provided an intermission in the debate the dispute between fisher and neyman terminated (unresolved after 27 years) with fisher's death in 1962 neyman wrote a well-regarded eulogy some of neyman's later publications reported p-values and significance levels
the modern version of hypothesis testing is a hybrid of the two approaches that resulted from confusion by writers of statistical textbooks (as predicted by fisher) beginning in the 1940s (but signal detection for example still uses the neyman/pearson formulation.) great conceptual differences and many caveats in addition to those mentioned above were ignored neyman and pearson provided the stronger terminology the more rigorous mathematics and the more consistent philosophy but the subject taught today in introductory statistics has more similarities with fisher's method than theirs this history explains the inconsistent terminology (example: the null hypothesis is never accepted but there is a region of acceptance)
sometime around 1940 in an apparent effort to provide researchers with a "non-controversial" way to have their cake and eat it too the authors of statistical text books began anonymously combining these two strategies by using the p-value in place of the test statistic (or data) to test against the neymanpearson "significance level" thus researchers were encouraged to infer the strength of their data against some null hypothesis using p-values while also thinking they are retaining the post-data collection objectivity provided by hypothesis testing it then became customary for the null hypothesis which was originally some realistic research hypothesis to be used almost solely as a strawman "nil" hypothesis (one where a treatment has no effect regardless of the context)
a comparison between fisherian frequentist (neymanpearson)
=== early choices of null hypothesis ===
paul meehl has argued that the epistemological importance of the choice of null hypothesis has gone largely unacknowledged when the null hypothesis is predicted by theory a more precise experiment will be a more severe test of the underlying theory when the null hypothesis defaults to "no difference" or "no effect" a more precise experiment is a less severe test of the theory that motivated performing the experiment an examination of the origins of the latter practice may therefore be useful:
1778: pierre laplace compares the birthrates of boys and girls in multiple european cities he states: "it is natural to conclude that these possibilities are very nearly in the same ratio" thus laplace's null hypothesis that the birthrates of boys and girls should be equal given "conventional wisdom"
1900: karl pearson develops the chi squared test to determine "whether a given form of frequency curve will effectively describe the samples drawn from a given population." thus the null hypothesis is that a population is described by some distribution predicted by theory he uses as an example the numbers of five and sixes in the weldon dice throw data
1904: karl pearson develops the concept of "contingency" in order to determine whether outcomes are independent of a given categorical factor here the null hypothesis is by default that two things are unrelated (e.g scar formation and death rates from smallpox) the null hypothesis in this case is no longer predicted by theory or conventional wisdom but is instead the principle of indifference that lead fisher and others to dismiss the use of "inverse probabilities"
== null hypothesis statistical significance testing vs hypothesis testing ==
an example of neymanpearson hypothesis testing can be made by a change to the radioactive suitcase example if the "suitcase" is actually a shielded container for the transportation of radioactive material then a test might be used to select among three hypotheses: no radioactive source present one present two (all) present the test could be required for safety with actions required in each case the neymanpearson lemma of hypothesis testing says that a good criterion for the selection of hypotheses is the ratio of their probabilities (a likelihood ratio) a simple method of solution is to select the hypothesis with the highest probability for the geiger counts observed the typical result matches intuition: few counts imply no source many counts imply two sources and intermediate counts imply one source
neymanpearson theory can accommodate both prior probabilities and the costs of actions resulting from decisions the former allows each test to consider the results of earlier tests (unlike fisher's significance tests) the latter allows the consideration of economic issues (for example) as well as probabilities a likelihood ratio remains a good criterion for selecting among hypotheses
the two forms of hypothesis testing are based on different problem formulations the original test is analogous to a true/false question; the neymanpearson test is more like multiple choice in the view of tukey the former produces a conclusion on the basis of only strong evidence while the latter produces a decision on the basis of available evidence while the two tests seem quite different both mathematically and philosophically later developments lead to the opposite claim consider many tiny radioactive sources the hypotheses become 0,1,2,3 grains of radioactive sand there is little distinction between none or some radiation (fisher) and 0 grains of radioactive sand versus all of the alternatives (neymanpearson) the major neymanpearson paper of 1933 also considered composite hypotheses (ones whose distribution includes an unknown parameter) an example proved the optimality of the (student's) t-test "there can be no better test for the hypothesis under consideration" (p 321) neymanpearson theory was proving the optimality of fisherian methods from its inception
fisher's significance testing has proven a popular flexible statistical tool in application with little mathematical growth potential neymanpearson hypothesis testing is claimed as a pillar of mathematical statistics creating a new paradigm for the field it also stimulated new applications in statistical process control detection theory decision theory and game theory both formulations have been successful but the successes have been of a different character
the dispute over formulations is unresolved science primarily uses fisher's (slightly modified) formulation as taught in introductory statistics statisticians study neymanpearson theory in graduate school mathematicians are proud of uniting the formulations philosophers consider them separately learned opinions deem the formulations variously competitive (fisher vs neyman) incompatible or complementary the dispute has become more complex since bayesian inference has achieved respectability
the terminology is inconsistent hypothesis testing can mean any mixture of two formulations that both changed with time any discussion of significance testing vs hypothesis testing is doubly vulnerable to confusion
fisher thought that hypothesis testing was a useful strategy for performing industrial quality control however he strongly disagreed that hypothesis testing could be useful for scientists hypothesis testing provides a means of finding test statistics used in significance testing the concept of power is useful in explaining the consequences of adjusting the significance level and is heavily used in sample size determination the two methods remain philosophically distinct they usually (but not always) produce the same mathematical answer the preferred answer is context dependent while the existing merger of fisher and neymanpearson theories has been heavily criticized modifying the merger to achieve bayesian goals has been considered
== criticism ==
criticism of statistical hypothesis testing fills volumes citing 300400 primary references much of the criticism can be summarized by the following issues:
the interpretation of a p-value is dependent upon stopping rule and definition of multiple comparison the former often changes during the course of a study and the latter is unavoidably ambiguous (i.e "p values depend on both the (data) observed and on the other possible (data) that might have been observed but weren't")
confusion resulting (in part) from combining the methods of fisher and neymanpearson which are conceptually distinct
emphasis on statistical significance to the exclusion of estimation and confirmation by repeated experiments
rigidly requiring statistical significance as a criterion for publication resulting in publication bias most of the criticism is indirect rather than being wrong statistical hypothesis testing is misunderstood overused and misused
when used to detect whether a difference exists between groups a paradox arises as improvements are made to experimental design (e.g. increased precision of measurement and sample size) the test becomes more lenient unless one accepts the absurd assumption that all sources of noise in the data cancel out completely the chance of finding statistical significance in either direction approaches 100%
layers of philosophical concerns the probability of statistical significance is a function of decisions made by experimenters/analysts if the decisions are based on convention they are termed arbitrary or mindless while those not so based may be termed subjective to minimize type ii errors large samples are recommended in psychology practically all null hypotheses are claimed to be false for sufficiently large samples so "...it is usually nonsensical to perform an experiment with the sole aim of rejecting the null hypothesis." "statistically significant findings are often misleading" in psychology statistical significance does not imply practical significance and correlation does not imply causation casting doubt on the null hypothesis is thus far from directly supporting the research hypothesis
"[i]t does not tell us what we want to know" lists of dozens of complaints are available
critics and supporters are largely in factual agreement regarding the characteristics of null hypothesis significance testing (nhst): while it can provide critical information it is inadequate as the sole tool for statistical analysis successfully rejecting the null hypothesis may offer no support for the research hypothesis the continuing controversy concerns the selection of the best statistical practices for the near-term future given the (often poor) existing practices critics would prefer to ban nhst completely forcing a complete departure from those practices while supporters suggest a less absolute change
controversy over significance testing and its effects on publication bias in particular has produced several results the american psychological association has strengthened its statistical reporting requirements after review medical journal publishers have recognized the obligation to publish some results that are not statistically significant to combat publication bias and a journal (journal of articles in support of the null hypothesis) has been created to publish such results exclusively textbooks have added some cautions and increased coverage of the tools necessary to estimate the size of the sample required to produce significant results major organizations have not abandoned use of significance tests although some have discussed doing so
== alternatives ==
the numerous criticisms of significance testing do not lead to a single alternative a unifying position of critics is that statistics should not lead to a conclusion or a decision but to a probability or to an estimated value with a confidence interval rather than to an accept-reject decision regarding a particular hypothesis it is unlikely that the controversy surrounding significance testing will be resolved in the near future its supposed flaws and unpopularity do not eliminate the need for an objective and transparent means of reaching conclusions regarding studies that produce statistical results critics have not unified around an alternative other forms of reporting confidence or uncertainty could probably grow in popularity one strong critic of significance testing suggested a list of reporting alternatives: effect sizes for importance prediction intervals for confidence replications and extensions for replicability meta-analyses for generality none of these suggested alternatives produces a conclusion/decision lehmann said that hypothesis testing theory can be presented in terms of conclusions/decisions probabilities or confidence intervals "the distinction between the  approaches is largely one of reporting and interpretation."
on one "alternative" there is no disagreement: fisher himself said "in relation to the test of significance we may say that a phenomenon is experimentally demonstrable when we know how to conduct an experiment which will rarely fail to give us a statistically significant result." cohen an influential critic of significance testing concurred " don't look for a magic alternative to nhst [null hypothesis significance testing]  it doesn't exist." " given the problems of statistical induction we must finally rely as have the older sciences on replication." the "alternative" to significance testing is repeated testing the easiest way to decrease statistical uncertainty is by obtaining more data whether by increased sample size or by repeated tests nickerson claimed to have never seen the publication of a literally replicated experiment in psychology an indirect approach to replication is meta-analysis
bayesian inference is one proposed alternative to significance testing (nickerson cited 10 sources suggesting it including rozeboom (1960)) for example bayesian parameter estimation can provide rich information about the data from which researchers can draw inferences while using uncertain priors that exert only minimal influence on the results when enough data is available psychologist kruschke john k has suggested bayesian estimation as an alternative for the t-test alternatively two competing models/hypothesis can be compared using bayes factors bayesian methods could be criticized for requiring information that is seldom available in the cases where significance testing is most heavily used neither the prior probabilities nor the probability distribution of the test statistic under the alternative hypothesis are often available in the social sciences
advocates of a bayesian approach sometimes claim that the goal of a researcher is most often to objectively assess the probability that a hypothesis is true based on the data they have collected neither fisher's significance testing nor neymanpearson hypothesis testing can provide this information and do not claim to the probability a hypothesis is true can only be derived from use of bayes' theorem which was unsatisfactory to both the fisher and neymanpearson camps due to the explicit use of subjectivity in the form of the prior probability fisher's strategy is to sidestep this with the p-value (an objective index based on the data alone) followed by inductive inference while neymanpearson devised their approach of inductive behaviour
== philosophy ==
hypothesis testing and philosophy intersect inferential statistics which includes hypothesis testing is applied probability both probability and its application are intertwined with philosophy philosopher david hume wrote "all knowledge degenerates into probability." competing practical definitions of probability reflect philosophical differences the most common application of hypothesis testing is in the scientific interpretation of experimental data which is naturally studied by the philosophy of science
fisher and neyman opposed the subjectivity of probability their views contributed to the objective definitions the core of their historical disagreement was philosophical
many of the philosophical criticisms of hypothesis testing are discussed by statisticians in other contexts particularly correlation does not imply causation and the design of experiments hypothesis testing is of continuing interest to philosophers
== education ==
statistics is increasingly being taught in schools with hypothesis testing being one of the elements taught many conclusions reported in the popular press (political opinion polls to medical studies) are based on statistics an informed public should understand the limitations of statistical conclusions and many college fields of study require a course in statistics for the same reason an introductory college statistics class places much emphasis on hypothesis testing perhaps half of the course such fields as literature and divinity now include findings based on statistical analysis (see the bible analyzer) an introductory statistics class teaches hypothesis testing as a cookbook process hypothesis testing is also taught at the postgraduate level statisticians learn how to create good statistical test procedures (like z student's t f and chi-squared) statistical hypothesis testing is considered a mature area within statistics but a limited amount of development continues
the cookbook method of teaching introductory statistics leaves no time for history philosophy or controversy hypothesis testing has been taught as received unified method surveys showed that graduates of the class were filled with philosophical misconceptions (on all aspects of statistical inference) that persisted among instructors while the problem was addressed more than a decade ago and calls for educational reform continue students still graduate from statistics classes holding fundamental misconceptions about hypothesis testing ideas for improving the teaching of hypothesis testing include encouraging students to search for statistical errors in published papers teaching the history of statistics and emphasizing the controversy in a generally dry subject
== see also ==
== references ==
== further reading ==
lehmann e.l (1992) "introduction to neyman and pearson (1933) on the problem of the most efficient tests of statistical hypotheses" in: breakthroughs in statistics volume 1 (eds kotz s. johnson n.l.) springer-verlag isbn 0-387-94037-5 (followed by reprinting of the paper)
neyman j.; pearson e.s (1933) "on the problem of the most efficient tests of statistical hypotheses" philosophical transactions of the royal society a 231 (694706): 289337 doi:10.1098/rsta.1933.0009
== external links ==
hazewinkel michiel ed (2001) "statistical hypotheses verification of" encyclopedia of mathematics springer isbn 978-1-55608-010-4
wilson gonzlez georgina; kay sankaran (september 10 1997) "hypothesis testing" environmental sampling & monitoring primer virginia tech
bayesian critique of classical hypothesis testing
critique of classical hypothesis testing highlighting long-standing qualms of statisticians
dallal ge (2007) the little handbook of statistical practice (a good tutorial)
references for arguments for and against hypothesis testing
statistical tests overview: how to choose the correct statistical test
an interactive online tool to encourage understanding hypothesis testing
a non mathematical way to understand hypothesis testing
=== online calculators ===
mbastats confidence interval and hypothesis test calculators
